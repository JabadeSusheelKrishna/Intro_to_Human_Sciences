{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Goal:* Minimize the Rosenbrock function (see below) using Newton method. As mentioned in class, we use the Newton direction $-(\\nabla^2f)^{-1}\\nabla f$, evaluated at the present location.\n",
    "\n",
    "You will implement this in Python using popular numpy for numerical calculations, and matplotlib for plotting. \n",
    "Make sure to install these. For installing you need to install: python, pip. \n",
    "After that, you need to install these using \n",
    "pip3 install matplotlib\n",
    "pip3 install numpy\n",
    "\n",
    "On mac, you may prefer to use brew install xyz-package\n",
    "\n",
    "Try installing this using terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Rosenbrock function\n",
    "$$f(x,y)=10(y-x^2)^2 + (1-x)^2$$\n",
    "\n",
    "1. (2 marks) Compute gradient and\n",
    "$$\\nabla f = \\left[\\begin{array}{c}\n",
    "todo \\\\\\\n",
    "todo\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "\n",
    "2. (2 marks) Hessian\n",
    "$$\\nabla^2 f = \\left[\n",
    "\\begin{array}{c}\n",
    "todo & todo \\\\\\\n",
    "todo & todo\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "3. (3 marks) Show that only minimum is at $(x,y)=(1,1)$ where $f(1,1)=0$. Show the optimality with first order and second order condition.\n",
    "4.  Prove or disprove that it is a convex function. Can you say a function is convex or concave using level curves only? See some level curves below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Got it, here's the revised version using double dollar signs for LaTeX equations:\n",
    "\n",
    "1. **Compute Gradient:**\n",
    "   To compute the gradient of the Rosenbrock function $f(x, y) $, we differentiate $f $ with respect to both $x $ and $y $:\n",
    "   $$ \n",
    "   \\nabla f = \\left[\\begin{array}{c}\n",
    "   \\frac{\\partial f}{\\partial x} \\\\\n",
    "   \\frac{\\partial f}{\\partial y}\n",
    "   \\end{array}\\right]\n",
    "   $$\n",
    "   We have:\n",
    "   $$ \n",
    "   \\frac{\\partial f}{\\partial x} = -40x(y-x^2) - 2(1-x)\n",
    "   $$\n",
    "   $$ \n",
    "   \\frac{\\partial f}{\\partial y} = 20(y-x^2)\n",
    "   $$\n",
    "\n",
    "2. **Compute Hessian:**\n",
    "   The Hessian matrix is the matrix of second-order partial derivatives of \\( f(x, y) \\). We have:\n",
    "   $$ \n",
    "   \\nabla^2 f = \\left[\n",
    "   \\begin{array}{cc}\n",
    "   \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\\n",
    "   \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2}\n",
    "   \\end{array}\\right]\n",
    "   $$\n",
    "   where\n",
    "   $$ \n",
    "   \\frac{\\partial^2 f}{\\partial x^2} = 120x^2 - 40y + 2\n",
    "   $$\n",
    "   $$ \n",
    "   \\frac{\\partial^2 f}{\\partial x \\partial y} = -40x\n",
    "   $$\n",
    "   $$ \n",
    "   \\frac{\\partial^2 f}{\\partial y^2} = 20\n",
    "   $$\n",
    "\n",
    "3. **Optimality at (1, 1):**\n",
    "   To show that the only minimum is at $$ (x, y) = (1, 1) $$ where $$ f(1, 1) = 0 $$ we can use the first and second-order conditions for optimality.\n",
    "   - first submit x and y values in Gradient. You will it becoming zero. (Which means the minimum is attained this can be Local minimum or Global minimum)\n",
    "   - Now submit x and y in Hessian. You will get matrix [[82 -40][-40 20]] which is semi positive definite. Here the minimum we got is global and optimum.\n",
    "\n",
    "4. **Convexity Analysis:**\n",
    "   We can determine if the function is convex or concave using its level curves. If all level curves are convex (concave upwards), the function is convex. If all level curves are concave (concave downwards), the function is concave. However, if the level curves are a mix of convex and concave sections, the function is neither convex nor concave. From the level curves, we can assess the function's behavior and infer its convexity or concavity.\n",
    "\n",
    "   -------------------------------------------------------------------\n",
    "To prove that the Rosenbrock function is neither convex nor concave, let's consider the definition of convexity:\n",
    "\n",
    "A function f(x) defined on an interval $ I $ is convex if for any $ x_1, x_2 \\in I $ and $ \\lambda \\in [0, 1] $, we have:\n",
    "\n",
    "$$ f(\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2) $$\n",
    "\n",
    "Now, let's examine the Rosenbrock function $ f(x, y) = (a - x)^2 + b(y - x^2)^2 $. \n",
    "\n",
    "We choose $ x_1 = (0, 0) $, $ x_2 = (1, 1) $, and $ \\lambda = 0.5 $:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\lambda x_1 + (1 - \\lambda) x_2) &= f(0.5, 0.5) = (a - 0.5)^2 + b(0.5 - 0.25)^2 \\\\\n",
    "\\lambda f(x_1) + (1 - \\lambda) f(x_2) &= 0.5 f(0, 0) + 0.5 f(1, 1) = 0.5 [(a - 0)^2 + b(0 - 0)^2] + 0.5 [(a - 1)^2 + b(1 - 1)^2]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For the function to be convex, we would need:\n",
    "\n",
    "$$\n",
    "(a - 0.5)^2 + b(0.5 - 0.25)^2 \\leq 0.5[(a - 0)^2 + b(0 - 0)^2] + 0.5[(a - 1)^2 + b(1 - 1)^2]\n",
    "$$\n",
    "\n",
    "Expanding and simplifying both sides, we would find a condition to hold for all \\( a \\) and \\( b \\) for the function to be convex. However, for the general Rosenbrock function, this condition won't hold, indicating that it's not convex everywhere.\n",
    "\n",
    "Similarly, we can analyze concavity, but for the general Rosenbrock function, it's generally not concave either.\n",
    "\n",
    "So, in conclusion, the Rosenbrock function is neither convex nor concave everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: In the following, you are required to retun the function values, gradient, and hessian. Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objfun(x,y):\n",
    "    term1 = 10 * ((y - (x**2))**2)\n",
    "    term2 = (1 - x)**2\n",
    "    return term1 + term2\n",
    "def gradient(x,y):\n",
    "    two_one_matrix = []\n",
    "    up = (40*(x**3)) - (40*x*y) + (2*x) - 2\n",
    "    down = (20 * (y - (x**2)))\n",
    "    two_one_matrix.append(up)\n",
    "    two_one_matrix.append(down)\n",
    "    return two_one_matrix\n",
    "def hessian(x,y):\n",
    "    matrix =[[],[]]\n",
    "    matrix[0].append((120*(x**2)) - (40*y) + 2)\n",
    "    matrix[0].append(-40*x)\n",
    "    matrix[1].append(-40*x)\n",
    "    matrix[1].append(20)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a utility function that plots the contours of the Rosenbrock function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def contourplot(objfun, xmin, xmax, ymin, ymax, ncontours=100, fill=True):\n",
    "\n",
    "    x = np.linspace(xmin, xmax, 200)\n",
    "    y = np.linspace(ymin, ymax, 200)\n",
    "    X, Y = np.meshgrid(x,y)\n",
    "    Z = objfun(X,Y)\n",
    "    if fill:\n",
    "        plt.contourf(X,Y,Z,ncontours); # plot the contours\n",
    "    else:\n",
    "        plt.contour(X,Y,Z,ncontours); # plot the contours\n",
    "    plt.scatter(1,1,marker=\"x\",s=50,color=\"r\");  # mark the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a contour plot of the Rosenbrock function, with the global minimum marked with a red cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contourplot(objfun, -30,30, -100, 300, fill=False)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Contours of $f(x,y)=10(y-x^2)^2 + (1-x)^2$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line search with Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we write a function that uses Newton's method to minimize a given objective function. Starts the solution at position `init`, moves along the Newton direction until the absolute difference between function values drops below `tolerance` or until the number of iterations exceeds `maxiter`.\n",
    "\n",
    "The step length $\\alpha$ is not used here, effectively set to 1.\n",
    "\n",
    "For efficiency, we use `np.linalg.solve` to determine the descent direction, instead of inverting the Hessian matrix. Inversion is no big deal in this 2D system, but that's a good habit to follow.\n",
    "\n",
    "The function returns the array of all intermediate positions, and the array of function values.\n",
    "\n",
    "(5 marks) Question: Fill the TODO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def newton(objfun, gradient, hessian, init, tolerance=1e-6, maxiter=10000, step_length = 1):\n",
    "    p = init\n",
    "    iterno = 0\n",
    "    parray = [p]\n",
    "    fprev = objfun(p[0],p[1])\n",
    "    farray = [fprev]\n",
    "    while iterno < maxiter:\n",
    "        g = gradient(p[0], p[1])\n",
    "        h = hessian(p[0], p[1])\n",
    "        p = p - (np.linalg.solve(h, g) * step_length)\n",
    "        fcur = objfun(p[0], p[1])\n",
    "        if np.isnan(fcur):\n",
    "            break\n",
    "        parray.append(p)\n",
    "        farray.append(fcur)\n",
    "        if abs(fcur-fprev)<tolerance:\n",
    "            break\n",
    "        fprev = fcur\n",
    "        iterno += 1\n",
    "    return np.array(parray), np.array(farray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how Newton's method behaves with the Rosenbrock function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "p, f = newton(objfun, gradient, hessian, init=[2,4])\n",
    "p1, f1 = newton(objfun, gradient, hessian, init=[2,4], step_length = 0.1)\n",
    "p2, f2 = newton(objfun, gradient, hessian, init=[2,4], step_length = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the convergence of the solution. Left: The solution points (white) superposed on the contour plot. The star indicates the initial point. Right: The objective function value at each iteration.\n",
    "\n",
    "Question: Do plots for learning rates: 1, 0.1, 0.01. Save plots. For which learning rates it converges to minima faster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -1,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ (Step_length = 1)\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -1,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ (Step_length = 0.1)\");\n",
    "plt.scatter(p1[0,0],p1[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p1)):    \n",
    "        plt.plot( (p1[i-1,0],p1[i,0]), (p1[i-1,1],p1[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f1)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -1,3,0,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ Step_length = 0.01\");\n",
    "plt.scatter(p2[0,0],p2[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p2)):    \n",
    "        plt.plot( (p2[i-1,0],p2[i,0]), (p2[i-1,1],p2[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f2)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The minimum is found in only three iterations. for step_length = 1\n",
    "- The minimum is found in sixty iterations. for step_length = 0.1\n",
    "- The minimum is found in 500 iterations. for step_length = 0.01\n",
    "\n",
    "Now let's start at a more difficult location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, f = newton(objfun, gradient, hessian, init=[-2,10])\n",
    "p1, f1 = newton(objfun, gradient, hessian, init=[-2,10], step_length=0.1)\n",
    "p2, f2 = newton(objfun, gradient, hessian, init=[-2,10], step_length=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -3,3,-10,10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ Step_length = 1\");\n",
    "plt.scatter(p[0,0],p[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p)):    \n",
    "        plt.plot( (p[i-1,0],p[i,0]), (p[i-1,1],p[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -5,5,-15,15)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ Step_Length = 0.1\");\n",
    "plt.scatter(p1[0,0],p1[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p1)):    \n",
    "        plt.plot( (p1[i-1,0],p1[i,0]), (p1[i-1,1],p1[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f1)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,5))\n",
    "plt.subplot(1,2,1)\n",
    "contourplot(objfun, -6,6,-30,30)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Minimize $f(x,y)=10(y-x^2)^2 + (1-x)^2$ Step_length = 0.01\");\n",
    "plt.scatter(p2[0,0],p2[0,1],marker=\"*\",color=\"w\")\n",
    "for i in range(1,len(p2)):    \n",
    "        plt.plot( (p2[i-1,0],p2[i,0]), (p2[i-1,1],p2[i,1]) , \"w\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(f2)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"function value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The minimum is found in only six iterations. for step_length = 1\n",
    "- The minimum is found in One Hundred and Fourty iterations. for step_length = 0.1\n",
    "- The minimum is found in Twelve Hundred iterations. for step_length = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using line search with Newton's method is much more successful compared to steepest descent. It finds the minimum in only a few iterations, and it does not require a step length parameter. The downside is that it requires the knowledge of the Hessian of the objective function. The Hessian is frequently either not available (lack of a closed form for the function, difficult to differentiate, etc.), or it is very costly to evaluate (especially in high dimensions)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
